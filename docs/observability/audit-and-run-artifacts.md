# Audit and Run Artifacts

**Status**: Mandatory  
**Enforcement**: Pipeline runs without these artifacts are considered incomplete  
**Last Updated**: 2026-01-08

## Purpose

This document defines the **audit artifacts** that every CI/CD pipeline run must generate and preserve. These artifacts enable:

- **Post-mortem debugging** without re-running pipelines
- **Compliance auditing** with immutable run records
- **Traceability** from deployment back to source code
- **Forensic analysis** of failures and security incidents
- **Operational confidence** through complete run history

## Core Principle

> **"A pipeline run without audit artifacts is incomplete."**

Every successful, failed, or cancelled pipeline execution must leave behind a complete record of what happened, why, and what was produced.

---

## Required Artifacts

### 1. Run Summary

**Artifact Name**: `pipeline-run-summary.json`  
**Format**: JSON  
**Generated by**: `pipeline-summary` job  
**Uploaded**: Always (success, failure, or cancellation)

**Purpose**: Complete record of the pipeline execution, including stage results, timings, and outcomes.

**Schema**:
```json
{
  "pipeline": {
    "name": "CI/CD Pipeline Skeleton",
    "version": "1.0.0",
    "run_id": "1234567890",
    "run_number": 42,
    "run_attempt": 1,
    "run_url": "https://github.com/org/repo/actions/runs/1234567890"
  },
  "trigger": {
    "event": "push",
    "branch": "feature/add-metrics",
    "commit_sha": "abc123...",
    "commit_message": "Add observability metrics",
    "author": "user@example.com",
    "timestamp": "2026-01-08T20:00:00Z"
  },
  "execution": {
    "start_time": "2026-01-08T20:00:05Z",
    "end_time": "2026-01-08T20:14:32Z",
    "duration_seconds": 867,
    "result": "success",
    "failure_stage": null
  },
  "stages": {
    "validate": {
      "executed": true,
      "result": "success",
      "duration_seconds": 12,
      "start_time": "2026-01-08T20:00:10Z",
      "end_time": "2026-01-08T20:00:22Z"
    },
    "build": {
      "executed": true,
      "result": "success",
      "duration_seconds": 145,
      "start_time": "2026-01-08T20:00:25Z",
      "end_time": "2026-01-08T20:02:50Z",
      "artifact_tag": "sha-abc123",
      "artifact_version": "1.0.0-abc123"
    },
    "test": {
      "executed": true,
      "result": "success",
      "duration_seconds": 62,
      "tests_total": 247,
      "tests_passed": 247,
      "tests_failed": 0
    },
    "scan": {
      "executed": true,
      "result": "success",
      "duration_seconds": 54,
      "vulnerabilities_critical": 0,
      "vulnerabilities_high": 2,
      "vulnerabilities_medium": 5
    },
    "deploy-dev": {
      "executed": true,
      "result": "success",
      "duration_seconds": 43,
      "environment": "development",
      "deployment_url": "https://dev.example.com"
    },
    "verify-dev": {
      "executed": true,
      "result": "success",
      "duration_seconds": 78,
      "health_checks_passed": 2,
      "smoke_tests_passed": 3
    }
  },
  "promotion": {
    "decisions": [
      {
        "from": "build",
        "to": "development",
        "decision": "allowed",
        "reason": "Feature branch → dev auto-deploy"
      }
    ]
  },
  "artifacts": {
    "image_tag": "sha-abc123",
    "image_registry": "myregistry.azurecr.io",
    "artifact_version": "1.0.0-abc123",
    "metadata_uploaded": true
  }
}
```

**Why it matters**: Single source of truth for what happened in the pipeline. Enables queries like:
- "What was the average build time last week?"
- "How many pipelines failed at the test stage?"
- "Which deployments went to staging on Jan 8?"

---

### 2. Artifact Metadata Snapshot

**Artifact Name**: `artifact-metadata-manifest.json`  
**Format**: JSON  
**Generated by**: `build` stage  
**Uploaded**: Always (when build succeeds)

**Purpose**: Immutable snapshot of the artifact's metadata for traceability.

**Schema**: (See [docs/artifacts/artifact-standards.md](../artifacts/artifact-standards.md))

**Example**:
```json
{
  "artifact": {
    "type": "docker-image",
    "name": "api-gateway",
    "version": "1.0.0-abc123",
    "tag": "sha-abc123",
    "registry": "myregistry.azurecr.io",
    "digest": "sha256:def456..."
  },
  "source": {
    "repository": "https://github.com/org/repo",
    "commit_sha": "abc123...",
    "branch": "feature/add-metrics",
    "commit_message": "Add observability metrics",
    "commit_author": "user@example.com",
    "commit_timestamp": "2026-01-08T19:45:00Z"
  },
  "build": {
    "pipeline_run_id": "1234567890",
    "pipeline_run_url": "https://github.com/org/repo/actions/runs/1234567890",
    "build_timestamp": "2026-01-08T20:02:50Z",
    "builder": "github-actions"
  }
}
```

**Why it matters**: 
- Enables tracing from deployed artifact back to exact source commit
- Supports "what version is deployed where?" queries
- Required for compliance and security audits

---

### 3. Promotion Decision Record

**Artifact Name**: `promotion-decisions.json`  
**Format**: JSON  
**Generated by**: Deployment stages  
**Uploaded**: Always (when deployments are attempted)

**Purpose**: Record of what artifacts were promoted to which environments, and which promotions were blocked.

**Schema**:
```json
{
  "run_id": "1234567890",
  "commit_sha": "abc123...",
  "artifact_tag": "sha-abc123",
  "decisions": [
    {
      "timestamp": "2026-01-08T20:05:00Z",
      "from": "build",
      "to": "development",
      "decision": "allowed",
      "reason": "Feature branch → dev auto-deploy",
      "executed": true,
      "deployment_result": "success"
    },
    {
      "timestamp": "2026-01-08T20:10:00Z",
      "from": "development",
      "to": "staging",
      "decision": "blocked",
      "reason": "Feature branch not eligible for staging",
      "executed": false,
      "deployment_result": null
    }
  ]
}
```

**Why it matters**:
- Answers "why didn't this deploy to production?"
- Tracks promotion gate effectiveness
- Audit trail for environment access

---

### 4. Approval Record (Production Only)

**Artifact Name**: `production-approval-record.json`  
**Format**: JSON  
**Generated by**: `deploy-production` stage  
**Uploaded**: Only when production deployment occurs

**Purpose**: Immutable record of who approved production deployment and when.

**Schema**:
```json
{
  "deployment": {
    "run_id": "1234567890",
    "artifact_tag": "sha-abc123",
    "environment": "production",
    "deployment_timestamp": "2026-01-08T15:30:00Z"
  },
  "approval": {
    "approver": "jane.doe@example.com",
    "approval_timestamp": "2026-01-08T15:28:45Z",
    "approval_type": "manual",
    "approval_comment": "Emergency hotfix for payment gateway"
  },
  "context": {
    "trigger_event": "workflow_dispatch",
    "trigger_user": "john.smith@example.com",
    "staging_verified": true,
    "staging_verification_timestamp": "2026-01-08T15:15:00Z"
  }
}
```

**Why it matters**:
- Compliance requirement: who authorized production changes
- Incident response: "who approved the deployment that caused the outage?"
- Audit trail for regulatory compliance

---

### 5. Verification Results

**Artifact Name**: `verification-result-{environment}.json`  
**Format**: JSON  
**Generated by**: `verify-dev`, `verify-staging`, `verify-production` stages  
**Uploaded**: Always (when verification runs)

**Purpose**: Record of post-deployment health checks and smoke tests.

**Schema**:
```json
{
  "environment": "development",
  "artifact": "myregistry.azurecr.io/api-gateway:sha-abc123",
  "verification_status": "PASSED",
  "verification_timestamp": "2026-01-08T20:07:15Z",
  "duration_seconds": 78,
  "health_checks": {
    "liveness": {
      "status": "PASSED",
      "endpoint": "/health/live",
      "response_code": 200,
      "duration_ms": 45
    },
    "readiness": {
      "status": "PASSED",
      "endpoint": "/health/ready",
      "response_code": 200,
      "duration_ms": 120
    }
  },
  "smoke_tests": {
    "health_endpoint": {
      "status": "PASSED",
      "duration_ms": 50
    },
    "api_endpoint": {
      "status": "PASSED",
      "duration_ms": 95
    },
    "authenticated_call": {
      "status": "PASSED",
      "duration_ms": 150
    }
  },
  "dependencies": {
    "database": {
      "status": "UP",
      "response_time_ms": 15
    },
    "cache": {
      "status": "UP",
      "response_time_ms": 8
    }
  }
}
```

**Why it matters**:
- Post-deployment validation proof
- Debugging failed verifications without re-running
- Baseline for performance regression detection

---

### 6. Failure Diagnostics (When Applicable)

**Artifact Name**: `failure-diagnostics.json`  
**Format**: JSON  
**Generated by**: Failed stage  
**Uploaded**: Only when pipeline fails

**Purpose**: Captured error details for post-mortem analysis.

**Schema**:
```json
{
  "failure": {
    "stage": "test",
    "timestamp": "2026-01-08T20:03:45Z",
    "exit_code": 1,
    "duration_before_failure_seconds": 62
  },
  "error": {
    "message": "Test suite exited with code 1",
    "stderr": "FAIL: test_user_authentication (assertions failed: 3)",
    "failed_tests": [
      {
        "name": "test_user_authentication",
        "error": "AssertionError: Expected 200, got 401"
      }
    ]
  },
  "context": {
    "commit_sha": "abc123...",
    "branch": "feature/auth-fix",
    "artifact_tag": "sha-abc123"
  },
  "recommendations": [
    "Review test failures in test report artifact",
    "Check if authentication service is mocked correctly",
    "Verify environment variables in test configuration"
  ]
}
```

**Why it matters**:
- Enables debugging without console log archeology
- Structured error data for trend analysis
- Faster root cause identification

---

## Artifact Storage and Retention

### Storage Location

**Current (Day 8)**: GitHub Actions artifacts
- Automatically uploaded via `actions/upload-artifact@v4`
- Accessible from GitHub Actions UI
- Retention: 90 days (default, configurable)

**Future**: External artifact storage
- Azure Blob Storage / S3 for long-term retention
- Integration with compliance/audit systems
- Permanent retention for production deployment records

### Retention Policy

| Artifact Type | Retention Period | Rationale |
|---------------|------------------|-----------|
| Run summary | 1 year | Operational history, performance trends |
| Artifact metadata | Permanent (for production) | Compliance, traceability |
| Promotion decisions | 1 year | Audit compliance |
| Approval records | 7 years | Regulatory requirement (SOX, etc.) |
| Verification results | 90 days (dev/staging), 1 year (prod) | Debugging, regression analysis |
| Failure diagnostics | 90 days | Debugging recent failures |

---

## Artifact Naming Conventions

All artifacts follow this naming pattern:

```
{artifact-type}-{environment?}-{run-id}.json
```

**Examples**:
- `pipeline-run-summary-1234567890.json`
- `artifact-metadata-manifest-1234567890.json`
- `promotion-decisions-1234567890.json`
- `verification-result-dev-1234567890.json`
- `verification-result-production-1234567890.json`
- `production-approval-record-1234567890.json`
- `failure-diagnostics-1234567890.json`

**Why**: Enables easy querying and correlation by run ID.

---

## Implementation Checklist

Every pipeline implementation must:

- [ ] Generate `pipeline-run-summary.json` in final summary job
- [ ] Upload artifact metadata in build stage
- [ ] Record promotion decisions in deployment stages
- [ ] Capture approval record for production deployments
- [ ] Upload verification results from verify stages
- [ ] Generate failure diagnostics on pipeline failure
- [ ] Use GitHub Actions artifact upload for all artifacts
- [ ] Include run ID in all artifact filenames

---

## Querying Artifacts (Examples)

### Find all failed deployments to staging last week

```bash
# Download all run summaries
# Parse JSON for: stages.deploy-staging.result == "failure"
jq '.stages["deploy-staging"].result == "failure"' pipeline-run-summary-*.json
```

### Trace artifact from production back to source commit

```bash
# Get production deployment artifact tag
# Find artifact metadata with matching tag
# Extract commit SHA from metadata
jq '.artifact.tag' production-approval-record-*.json
jq --arg tag "$ARTIFACT_TAG" 'select(.artifact.tag == $tag) | .source.commit_sha' artifact-metadata-*.json
```

### Identify most common failure stage

```bash
# Extract failure_stage from all run summaries
jq -r '.execution.failure_stage' pipeline-run-summary-*.json | sort | uniq -c | sort -rn
```

---

## Compliance and Audit Use Cases

### SOX Compliance

Audit artifacts satisfy SOX requirements for:
- Change traceability (artifact metadata)
- Approval evidence (approval records)
- Deployment history (run summaries)

### Security Incident Response

During a security incident:
1. Identify affected deployments via run summaries
2. Trace back to source commits via artifact metadata
3. Review approval records to identify authorization chain
4. Analyze verification results for anomaly detection

### Performance Regression Analysis

Compare verification results over time:
1. Extract health check response times from verification artifacts
2. Plot trends to identify degradation
3. Correlate with deployment timestamps to identify offending changes

---

## Non-Negotiable Principle

**Every pipeline run is incomplete without audit artifacts.**

If a pipeline run does not generate and upload the required artifacts, it is considered a **platform bug**, not a pipeline feature gap.

Audit artifacts are not "nice to have" — they are **foundational to platform credibility**.

---

## Related Documentation

- [pipeline-metrics.md](pipeline-metrics.md) - Metrics that must be measured
- [README.md](README.md) - Observability overview
- [../artifacts/artifact-standards.md](../artifacts/artifact-standards.md) - Artifact metadata format
- [../promotion/promotion-model.md](../promotion/promotion-model.md) - Promotion decision logic
